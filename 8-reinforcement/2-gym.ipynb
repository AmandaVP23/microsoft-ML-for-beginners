{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47c40f7",
   "metadata": {},
   "source": [
    "# CartPole Skating\n",
    "We will use a special **simulation environment**, which will simulate the physics behind the balancing pole.\n",
    "One of the most popular simulation environments for training reinforcement learning alrogithms is called a Gym, which was maintained by OpenAI.\n",
    "By using this gym we can create different **environments** from a cartpole simulation to Atari games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45c4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8842888",
   "metadata": {},
   "source": [
    "### Initialize a cartpole environment\n",
    "To work with a cartpole balancing problem, we need to initialize corresponding environment. Each environment is associated with an:\n",
    "\n",
    "- **Observation space** that defines the structure of information that we receive from the environment. For cartpole problem, we receive position of the pole, velocity and some other values.\n",
    "\n",
    "- **Action space** that defines possible actions. In our case the action space is discrete, and consists of two actions - **left** and **right**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c7eb80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334b39b",
   "metadata": {},
   "source": [
    "To see how the environment works lets run a short simulation for 100 steps. At each step, we providade one of the actions to be taken - in this simulation we just randomly select an action from `action_space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959868d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72b303",
   "metadata": {},
   "source": [
    "During simulation, we need to get observations in order to decide how to act. In fact, the step function returns current observations, a reward function, and the done flag that indicates whether it makes sense to continue the simulation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    done = terminated or truncated\n",
    "    print(f\"{obs} -> {rew}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0b04d",
   "metadata": {},
   "source": [
    "The observation vector that is returned at each step of the simulation contains the following values:\n",
    "\n",
    "    - Position of cart\n",
    "    - Velocity of cart\n",
    "    - Angle of pole\n",
    "    - Rotation rate of pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min and max value of those numbers\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e90f05",
   "metadata": {},
   "source": [
    "You may also notice that reward value on each simulation step is always 1. This is because our goal is to survive as long as possible, i.e. keep the pole to a reasonably vertical position for the longest period of time.\n",
    "\n",
    "✅ In fact, the CartPole simulation is considered solved if we manage to get the average reward of 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17dd918",
   "metadata": {},
   "source": [
    "### State discretization\n",
    "\n",
    "In Q-Learning, we need to build Q-Table that defines what to do at each state. To be able to do this, we need state to be discreet, more precisely, it should contain finite number of discrete values. Thus, we need somehow to discretize our observations, mapping them to a finite set of states.\n",
    "\n",
    "There are a few ways we can do this:\n",
    "\n",
    "- **Divide into bins**. If we know the interval of a certain value, we can divide this interval into a number of **bins**, and then replace the value by the bin number that it belongs to. This can be done using the numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) method. In this case, we will precisely know the state size, because it will depend on the number of bins we select for digitalization.\n",
    "\n",
    "✅ We can use linear interpolation to bring values to some finite interval (say, from -20 to 20), and then convert numbers to integers by rounding them. This gives us a bit less control on the size of the state, especially if we do not know the exact ranges of input values. For example, in our case 2 out of 4 values do not have upper/lower bounds on their values, which may result in the infinite number of states.\n",
    "\n",
    "In our example, we will go with the second approach. As you may notice later, despite undefined upper/lower bounds, those value rarely take values outside of certain finite intervals, thus those states with extreme values will be very rare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801160d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will take the observation from our model and produce a tuple of 4 integer values\n",
    "def discretize(x):\n",
    "    return tuple((x / np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7cf6f",
   "metadata": {},
   "source": [
    "Let's also explore another discretization method using bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ab656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(i, num):\n",
    "    return np.arrange(num + 1) * (i[1] - i[0]) / num + i[0]\n",
    "\n",
    "print(\"Sample bins for interval (-5, 5) with 10 bins\\n\", create_bins((-5, 5), 10))\n",
    "\n",
    "ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
    "nbins = [20,20,10,10] # number of bins for each parameter\n",
    "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
    "\n",
    "def discretize_bins(x):\n",
    "    return tuple(np.digitize(x[i], bins[i]) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f1b53",
   "metadata": {},
   "source": [
    "Lets run a short simulation and observe those discrete environment values. Feel free to try both `discretize` and `discretize_bins` and see if there is a difference\n",
    "\n",
    "    ✅ discretize_bins returns the bin number, which is 0-based. Thus for values of input variable around 0 it returns the number from the middle of the interval (10). In discretize, we did not care about the range of output values, allowing them to be negative, thus the state values are not shifted, and 0 corresponds to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498733f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    #env.render()\n",
    "    obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    done = terminated or truncated\n",
    "    #print(discretize_bins(obs))\n",
    "    print(discretize(obs))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
